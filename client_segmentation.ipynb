{"cells":[{"attachments":{},"cell_type":"markdown","id":"7d0c2dad-f6c8-48e7-9e55-acb49348228d","metadata":{"id":"7d0c2dad-f6c8-48e7-9e55-acb49348228d","jp-MarkdownHeadingCollapsed":true,"tags":[]},"source":["# Python Libraries"]},{"attachments":{},"cell_type":"markdown","id":"dc320cd8","metadata":{},"source":["Importing libraries, reaading the dataframe from the excel file, setting the seed for reproducibility and setting <code>ignore warnings</code>."]},{"cell_type":"code","execution_count":null,"id":"4ba6816f-3ca3-4f03-8c24-6ba2067841d2","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4483,"status":"ok","timestamp":1677882066963,"user":{"displayName":"Matteo Venturelli","userId":"04703003044878250847"},"user_tz":-60},"id":"4ba6816f-3ca3-4f03-8c24-6ba2067841d2","outputId":"6e897b8d-f056-4a02-be93-00844a47ae4d"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import warnings\n","import seaborn as sns\n","import xgboost\n","\n","df = pd.read_excel('BankClients.xlsx')\n","\n","seed = 69\n","\n","warnings.simplefilter(action='ignore', category=FutureWarning)\n","warnings.simplefilter(action='ignore', category=Warning)"]},{"attachments":{},"cell_type":"markdown","id":"ae476855-8308-4179-a4ea-397c7849b69f","metadata":{"id":"ae476855-8308-4179-a4ea-397c7849b69f","jp-MarkdownHeadingCollapsed":true,"tags":[]},"source":["# Data preprocessing"]},{"attachments":{},"cell_type":"markdown","id":"5e07ccd5","metadata":{},"source":["The first column (index) is deleted, since it is useless for clustering."]},{"cell_type":"code","execution_count":null,"id":"cd36188a-c22d-4c1c-b44b-7cf77811a3eb","metadata":{"executionInfo":{"elapsed":300,"status":"ok","timestamp":1677879379029,"user":{"displayName":"Matteo Venturelli","userId":"04703003044878250847"},"user_tz":-60},"id":"cd36188a-c22d-4c1c-b44b-7cf77811a3eb"},"outputs":[],"source":["df.drop(labels = 'ID', axis = 1, inplace = True)"]},{"attachments":{},"cell_type":"markdown","id":"48638a6d","metadata":{},"source":["The original dataframe is splitted into one containing the numerical features and a second one containing the categorical features."]},{"cell_type":"code","execution_count":null,"id":"50e574ea-b65f-4d87-b30c-145750cd0a26","metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1677879380527,"user":{"displayName":"Matteo Venturelli","userId":"04703003044878250847"},"user_tz":-60},"id":"50e574ea-b65f-4d87-b30c-145750cd0a26","tags":[]},"outputs":[],"source":["df_num = df[['Age', 'Income', 'Wealth', 'Debt', 'FinEdu', 'ESG', 'Digital', 'BankFriend', 'LifeStyle', 'Luxury', 'Saving', 'FamilySize']]\n","df_cat = df[['Gender', 'Job', 'Area', 'CitySize', 'Investments']]"]},{"attachments":{},"cell_type":"markdown","id":"20b8b6f8","metadata":{},"source":["Since the dataframe contains both categorical and numerical features, using classical distance metrics (e.g. Euclidean distance) would not be the best option. A mixed distance should be used, which takes into account the <code>Hamming</code> distance measure for categorical features and the <code>L1</code> distance mesure for numerical features. To use the <code>Hamming</code> distance measure, a one-hot encoding is needed. \n","\n","The <code>pandas</code> function <code>get_dummies</code> is used to generate one-hot encoded features, so that each column (i.e. feature) in the initial dataframe is converted into <code>n</code> columns, where <code>n</code> is the number of possible values of that feature. The final dataframe is constructed as the concatenation of the new one-hot encoded features with the numerical ones. "]},{"cell_type":"code","execution_count":null,"id":"81d89ad3-51b0-4e92-9f39-336c04381452","metadata":{"executionInfo":{"elapsed":271,"status":"ok","timestamp":1677879418535,"user":{"displayName":"Matteo Venturelli","userId":"04703003044878250847"},"user_tz":-60},"id":"81d89ad3-51b0-4e92-9f39-336c04381452","tags":[]},"outputs":[],"source":["# creating an empty dataframe for dummy variables\n","df_dummy = pd.DataFrame()\n","\n","# for each column of the dataframe containing the categorical features, the respective one hot-encoding dataframe (n columns)\n","# is created and concatenated to the one containing all of them. Columns need to be renamed after the encoding to prevent\n","# them to get the same name.\n","\n","for columns in df_cat.columns:\n","    df_temp = pd.get_dummies(df_cat[columns])\n","    df_temp.columns = [columns + str(x) for x in df_temp.columns]\n","    df_dummy = pd.concat([df_dummy, df_temp], axis = 1)\n","\n","df = pd.concat([df_dummy, df_num], axis = 1)"]},{"attachments":{},"cell_type":"markdown","id":"e8db807e","metadata":{},"source":["In order to facilitate the clustering, <code>MinMax</code> scaling is applied to the data (feature-wise)."]},{"cell_type":"code","execution_count":null,"id":"a51cd4de-8ada-48ad-84b7-4d3867cd3867","metadata":{"executionInfo":{"elapsed":362,"status":"ok","timestamp":1677879424676,"user":{"displayName":"Matteo Venturelli","userId":"04703003044878250847"},"user_tz":-60},"id":"a51cd4de-8ada-48ad-84b7-4d3867cd3867"},"outputs":[],"source":["for c in df_num.columns:\n","    df[c] = (df[c] - df[c].min()) / (df[c].max() - df[c].min())"]},{"attachments":{},"cell_type":"markdown","id":"5e3e5905","metadata":{},"source":["A <code>numpy</code> array is created starting from the dataframe."]},{"cell_type":"code","execution_count":null,"id":"29cda84f","metadata":{},"outputs":[],"source":["data = df.sample(frac = 1, random_state = seed).to_numpy()"]},{"attachments":{},"cell_type":"markdown","id":"0d26c309-c244-43e4-a829-b98fb7ae0340","metadata":{"id":"0d26c309-c244-43e4-a829-b98fb7ae0340","jp-MarkdownHeadingCollapsed":true,"tags":[]},"source":["# Data exploration"]},{"attachments":{},"cell_type":"markdown","id":"0bf6b0de","metadata":{},"source":["A summary of the statistical quantities related to the dataset shown."]},{"cell_type":"code","execution_count":null,"id":"65bde3b8","metadata":{},"outputs":[],"source":["df.describe()"]},{"attachments":{},"cell_type":"markdown","id":"d65691b2","metadata":{},"source":["### Missing value analysis."]},{"cell_type":"code","execution_count":null,"id":"562bc0e8","metadata":{},"outputs":[],"source":["df_na = (df.isnull().sum() / len(df)) * 100\n","df_na = df_na.drop(df_na[df_na == 0].index).sort_values(ascending=False)\n","missing_data = pd.DataFrame({'Missing Ratio' :df_na})\n","missing_data.head(10)"]},{"attachments":{},"cell_type":"markdown","id":"f2455405","metadata":{},"source":["No missing values are present in the dataset."]},{"attachments":{},"cell_type":"markdown","id":"1e504e19","metadata":{},"source":["### Correlation matrix analysis."]},{"cell_type":"code","execution_count":null,"id":"a00df8a0","metadata":{},"outputs":[],"source":["corrmat = df.corr()\n","plt.figure(figsize=(25, 25), dpi = 300)\n","sns.heatmap(corrmat, \n","            cbar=True, \n","            annot=True, \n","            square=True, \n","            cmap=\"Blues\",\n","            fmt='.2f', \n","            annot_kws={'size': 10}, \n","            yticklabels=df.columns, \n","            xticklabels=df.columns)"]},{"attachments":{},"cell_type":"markdown","id":"39542570","metadata":{},"source":["There is no evidence showing the precence of highly-correlated features in the dataset, indeed the correlation index does not take values larger than $0.6$. For this reason, the dataset will remain unchanged, at least throughout the preliminary analysis."]},{"attachments":{},"cell_type":"markdown","id":"c8b442d7","metadata":{},"source":["### Outliers analysis."]},{"attachments":{},"cell_type":"markdown","id":"c9c644ee","metadata":{},"source":["Each feature is analyzed separately using standard deviation as statistic. The points lying outside a threshold of $3\\sigma$ from the feature mean are labeled as outliers and are removed from the dataset."]},{"cell_type":"code","execution_count":null,"id":"da9e3865","metadata":{},"outputs":[],"source":["features = df.columns\n","X = df[features].values\n","\n","def find_outliers(X):\n","    for c in range(16,28):\n","        mean = X[:,c].mean()\n","        std = X[:,c].std()\n","        lower_bound = mean - 3*std\n","        upper_bound = mean + 3*std\n","        outliers = ((X[:,c]>upper_bound) | (X[:,c]<lower_bound))\n","        if (outliers.sum()>0):\n","            print(\"Feature %d (%.3f,%.3f) has %d outliers\"%(c,mean,std,outliers.sum()))\n","            print(\"\\t===============================\")\n","            for i,j in enumerate(X[outliers==True,c]):\n","                print(\"\\t%d\\t%.3f\"%(i,j))\n","            print(\"\\t===============================\")\n","            X = np.delete(X, np.where(outliers == True)[0], axis=0)\n","\n","        else:\n","            print(\"Feature %d has no outliers\"%c)\n","\n","        \n","    return X"]},{"cell_type":"code","execution_count":null,"id":"9b741931","metadata":{},"outputs":[],"source":["X = find_outliers(X)\n","df = pd.DataFrame(X)"]},{"attachments":{},"cell_type":"markdown","id":"b70fe89a","metadata":{},"source":["Seven outliers are found and removed from the dataset."]},{"attachments":{},"cell_type":"markdown","id":"8a1ba051","metadata":{},"source":["### PCA"]},{"attachments":{},"cell_type":"markdown","id":"ea5468ba","metadata":{},"source":["Principal component analysis (PCA) is a popular technique for analyzing large datasets containing a high number of dimensions/features per observation, increasing the interpretability of data while preserving the maximum amount of information, and enabling the visualization of multidimensional data.\n","\n","Here Principal Component Analysis is applied to numerical features in order to evaluate if feature reduction could be effective. Feature variance (in decreasing order) is computed before and after PCA, in addition to the 'cumulative variance'. 'Cumulative variance' of feature <code>i</code> is intended as the sum of variances from feature $1$ to feature <code>i</code>. \n","\n","Values are normalized by the sum of feature variances (trace of covariance matrix)."]},{"cell_type":"code","execution_count":null,"id":"1723471f","metadata":{},"outputs":[],"source":["from sklearn.decomposition import PCA\n","\n","# defining the numerical portion of data array (from 17th feature on, so starting from index 16)\n","num_data = data[:, 16:]\n","\n","# calculating covariance matrix, variance vector and cumulative sum vector of data before applying PCA\n","# the values are normalized in order to have the sum of feature variances equal to one\n","cov = np.cov(num_data.T) / np.trace(np.cov(num_data.T))\n","var = np.flip(np.sort(np.diag(cov)))        # np.flip is used since np.sort sorts the vector in increasing order\n","cumulative = np.empty(var.shape)\n","for i in range(var.shape[0]):\n","    cumulative[i] = sum(var[:i+1])\n","\n","# applying PCA and storing transformed data in pca_data\n","pca = PCA(n_components = num_data.shape[1], random_state = seed)\n","pca.fit(num_data)\n","pca_data = pca.transform(num_data)\n","\n","# calculating covariance matrix, variance vector and cumulative sum vector of data after applying PCA\n","pca_cov = np.cov(pca_data.T) / np.trace(np.cov(num_data.T))\n","pca_var = np.flip(np.sort(np.diag(pca_cov)))\n","pca_cumulative = np.empty(pca_var.shape)\n","for i in range(pca_var.shape[0]):\n","    pca_cumulative[i] = sum(pca_var[:i+1])\n","\n","# creating a vector containing the indexes of the numerical features used to plot PCA results\n","x = np.arange(1, var.shape[0] + 1)\n","\n","fig, axes = plt.subplots(1, 2, figsize=(18,6), dpi=80)\n","\n","# plotting the variances before and after PCA\n","ax = axes[0]\n","ax.plot(x, var, 'o--', color = 'blue', label = 'Before PCA')\n","ax.plot(x, pca_var, 'o--', color = 'orange', label = 'After PCA')\n","ax.set_xlabel('Feature number')\n","ax.set_ylabel('Variance')\n","ax.legend()\n","\n","# plotting the cumulative variances before and after PCA\n","ax = axes[1]\n","ax.plot(x, cumulative, 'o--', color = 'blue', label = 'Before PCA')\n","ax.plot(x, pca_cumulative, 'o--', color = 'orange', label = 'After PCA')\n","ax.set_xlabel('Feature number')\n","ax.set_ylabel('Cumulative variance')\n","ax.legend()\n","\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","id":"5475cac1","metadata":{},"source":["### Custom distance"]},{"attachments":{},"cell_type":"markdown","id":"0d289d7c","metadata":{},"source":["The function <code>mixed_distance</code> to compute distance is defined.\n","\n","<code>hamming</code> distance metric is used for categorical values and $L1$ (<code>cityblock</code>) is used for numerical values. The sum is weighted by the number of features **before** one-hot encoding, otherwise dummy variables would be considered."]},{"cell_type":"code","execution_count":null,"id":"L0Ll6Bc6dlv2","metadata":{"executionInfo":{"elapsed":247,"status":"ok","timestamp":1677881451931,"user":{"displayName":"Matteo Venturelli","userId":"04703003044878250847"},"user_tz":-60},"id":"L0Ll6Bc6dlv2"},"outputs":[],"source":["from scipy.spatial.distance import cdist\n","\n","def mixed_distance(A, B):\n","    nFeatures = 28      # total number of features (i.e. total number of columns of the array)\n","    nOneHot = 16        # number of one-hot encoded features (columns of the array with possible values 0/1)\n","    nCategorical = 5    # number of categorical features before one-hot encoding, used in the weighted sum to evaluate the distance\n","\n","    # reshaping is needed in order for the function to work\n","    A = A.reshape(1,-1)\n","    B = B.reshape(1,-1)\n","\n","    # computing the hamming distance for one-hot columns and cityblock (L1) distance for numerical columns\n","    dCategorical = cdist(A[:, :nOneHot], B[:, :nOneHot], 'hamming')\n","    dNumeric = cdist(A[:, nOneHot:], B[:, nOneHot:], 'cityblock')\n","\n","    # computing the weight associated to categorical features (number of categorical features divided by total number of features)\n","    wCategorical = float(nCategorical) / (float(nFeatures - nOneHot + nCategorical))\n","\n","    # computing the distance as the weighted sum of hamming and L1 distances\n","    distance = wCategorical * dCategorical + (1 - wCategorical) * dNumeric\n","    return distance"]},{"attachments":{},"cell_type":"markdown","id":"45838ad6","metadata":{},"source":["### t-SNE"]},{"attachments":{},"cell_type":"markdown","id":"51db7cc9","metadata":{},"source":["t-distributed stochastic neighbor embedding (t-SNE) is a statistical method for visualizing high-dimensional data by giving each datapoint a location in a two or three-dimensional map.\n","\n","There are two main hyperparameters for t-SNE (in addition to the dimensionality of the final representation), which are <code>perplexity</code> and <code>learning_rate</code>.\n","\n","<code>perplexity</code> defines the balance between local and global aspects of the data. The parameter is, in a sense, a guess about the number of close neighbors each point has. Usual values are in the range $\\left[ 5, 50\\right]$.\n","\n","<code>learning_rate</code> defines the step size at each iteration. If the <code>learning_rate</code> is too high, data may look like a ‘ball’ with any point approximately equidistant from its nearest neighbours. If the <code>learning_rate</code> is too low, most points may look compressed in a dense cloud with few outliers. Usual values are in the range $\\left[ 10, 1000\\right]$.\n","\n","Initially the <code>learning_rate</code> is set to the default value $\\left( 200\\right)$, and t-SNE is applied with different <code>perplexity</code> values. Then we fix the <code>perplexity</code> to the value which yielded the best results (in terms of effectively separating data) and apply tSNE with different <code>learning_rate</code> values."]},{"cell_type":"code","execution_count":null,"id":"7fc49929-b9d6-4e12-bde2-5277290d1d2b","metadata":{"id":"7fc49929-b9d6-4e12-bde2-5277290d1d2b"},"outputs":[],"source":["from sklearn.manifold import TSNE\n","\n","# usual values for perplexity are in the range (5, 50) so we define a range (5, 55) with step size = 10\n","initial_perplexity = 5\n","final_perplexity = 55\n","step = 10\n","perplexity_vector = np.arange(initial_perplexity, final_perplexity + step, step)\n","\n","learning_rate = 200   # default value\n","dimensions = 2        # output dimensionality\n","\n","# defining rows and columns of the subplots, then defining the matplotlib object to contain multiple plots\n","row = 0\n","column = 0\n","max_rows = 2\n","max_columns = 3\n","fig, axes = plt.subplots(max_rows, max_columns, figsize=(18,8), dpi=80)\n","\n","# for each perplexity value we apply t-sne and plot the results\n","for perplexity in perplexity_vector:\n","  tsne = TSNE(n_components=dimensions,\n","              perplexity=perplexity,\n","              learning_rate=learning_rate,\n","              metric=mixed_distance,        # the metric used is the custom mixed_distance\n","              random_state=seed)\n","  \n","  # using fit_transform to apply the transformation on the data and store the new representation in X_tsne\n","  X_tsne = tsne.fit_transform(data)\n","\n","  # defining the subplot to plot data on\n","  ax = axes[row][column]\n","\n","  # plotting the data in a scatterplot\n","  ax.scatter(X_tsne[:, 0], X_tsne[:, 1])\n","  ax.set_title(f'Perplexity = {perplexity}')\n","\n","  # updating column and row values\n","  column = column + 1\n","  if column % max_columns == 0:\n","    column = 0\n","    row = row + 1 "]},{"attachments":{},"cell_type":"markdown","id":"c3cd1ae8","metadata":{},"source":["Now <code>perplexity</code> is fixed and different values of <code>learning_rate</code> are considered."]},{"cell_type":"code","execution_count":null,"id":"418894ce","metadata":{},"outputs":[],"source":["# setting learning_rate to vary between 80 and 520 with step size of 40\n","initial_learning_rate = 40\n","final_learning_rate = 440\n","step = 80\n","learning_rate_vector = np.arange(initial_learning_rate, final_learning_rate + step, step)\n","\n","perplexity = 45   # 'best' value from past cell\n","dimensions = 2\n","\n","row = 0\n","column = 0\n","max_rows = 2\n","max_columns = 3\n","fig, axes = plt.subplots(max_rows, max_columns, figsize=(18,8), dpi=80)\n","\n","for learning_rate in learning_rate_vector:\n","  tsne = TSNE(n_components=dimensions,\n","              perplexity=perplexity,\n","              learning_rate=learning_rate,\n","              metric=mixed_distance,\n","              random_state=seed)\n","  \n","  X_tsne = tsne.fit_transform(data)\n","\n","  ax = axes[row][column]\n","\n","  ax.scatter(X_tsne[:, 0], X_tsne[:, 1])\n","  ax.set_title(f'Learning rate = {learning_rate}')\n","  \n","  column = column + 1\n","  if column % 3 == 0:\n","    column = 0\n","    row = row + 1 "]},{"attachments":{},"cell_type":"markdown","id":"a3e71ad4-7559-456a-ad91-87f79c7d8f06","metadata":{"id":"a3e71ad4-7559-456a-ad91-87f79c7d8f06","jp-MarkdownHeadingCollapsed":true,"tags":[]},"source":["# Clustering"]},{"attachments":{},"cell_type":"markdown","id":"b682666c","metadata":{},"source":["Several clustering techniques are tried out: k-medoids, DBSCAN, OPTICS and Gaussian Mixture Model. To evaluate the performance of these algorithms, three different scores are used:\n","\n","- The *silhouette score* is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation). The score is calculated using the mean intra-cluster distance (<code>a</code>) and the mean nearest-cluster distance (<code>b</code>) for each sample. The silhouette score for a sample is <code>(b - a) / max(a, b)</code>. To clarify, <code>b</code> is the distance between a sample and the nearest cluster that the sample is not a part of. The best value is $1$ and the worst value is $-1$. Values near $0$ indicate overlapping clusters. Negative values generally indicate that a sample has been assigned to the wrong cluster, as a different cluster is more similar.\n","\n","- The *Calinski-Harabasz* (CH) *score*, also known as Variance ratio criterion, is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation). Cohesion is estimated based on the distances from the data points in a cluster to its cluster centroid and separation is based on the distance of the cluster centroids from the global centroid. Higher value of CH score means the clusters are dense and well separated, although there is no 'acceptable' cut-off value. The value to be chosen is the one that gives a peak, or at least an abrupt elbow, on the line plot of CH indices. On the other hand, if the line is smooth (horizontal or ascending or descending) then there is no such reason to prefer one solution over others.\n","\n","- The *Davies-Boulding* (DB) *score* is defined as the average similarity measure of each cluster with its most similar cluster, where similarity is the ratio of within-cluster distances to between-cluster distances. Thus, clusters which are farther apart and less dispersed will result in a better score.The minimum score is zero, with lower values indicating better clustering.\n","\n","These scores are used in model selection to choose the optimum values for the hyperparameters of each clustering technique. Results are then plotted in 2D t-SNE representation."]},{"attachments":{},"cell_type":"markdown","id":"8138c261","metadata":{},"source":["## Subsampling"]},{"attachments":{},"cell_type":"markdown","id":"cb211ad9","metadata":{},"source":["A random subsample of data is selected to shorten the computation time. <code>frac = 0.2</code> reduces the original $5000$ samples to $1000$, which should still be enough to perform clustering effectively. \n","\n","The dataframe is converted into an array."]},{"cell_type":"code","execution_count":null,"id":"653c3ad5","metadata":{},"outputs":[],"source":["data = df.sample(frac = 0.2, random_state = seed).to_numpy()"]},{"attachments":{},"cell_type":"markdown","id":"fc912216","metadata":{},"source":["## k-medoids clustering "]},{"attachments":{},"cell_type":"markdown","id":"3627401e","metadata":{},"source":["k-medoids clustering technique attempts to minimize the distance between points labeled to be in a cluster and a point designated as the center of that cluster, called medoid. The algorithm starts by randomly selecting <code>k</code> medoids from the dataset, where <code>k</code> is the number of clusters desired. Then, each data point is assigned to the nearest medoid based on a distance metric (<code>mixed_distance</code> in this case). The algorithm then iteratively updates the medoids by trying to minimize the sum of distances between each data point and its assigned medoid.\n","\n","<code>k</code> values between <code>k_min = 3</code> and <code>k_max = 10</code> are tried out. The scores are computed and stored for each <code>k</code> value."]},{"cell_type":"code","execution_count":null,"id":"bwR8dNteaLYe","metadata":{"executionInfo":{"elapsed":21718,"status":"ok","timestamp":1677881475845,"user":{"displayName":"Matteo Venturelli","userId":"04703003044878250847"},"user_tz":-60},"id":"bwR8dNteaLYe"},"outputs":[],"source":["from sklearn_extra.cluster import KMedoids\n","from sklearn.metrics import silhouette_score\n","from sklearn.metrics import calinski_harabasz_score\n","from sklearn.metrics import davies_bouldin_score\n","\n","k_min = 3\n","k_max = 10\n","\n","# defining the vector containing the possible values of k\n","k_vector = np.arange(k_min, k_max + 1, 1)\n","\n","# defining the empty vectors of the scores\n","silhouette = np.empty(k_vector.shape)\n","ch_score = np.empty(k_vector.shape)\n","db_score = np.empty(k_vector.shape)\n","\n","# for each k value, run the k-medoids algorithm and store the values of the scores\n","for k in k_vector:\n","    kmedoids = KMedoids(n_clusters = k, metric = mixed_distance, random_state = seed).fit(data)\n","    silhouette[k - k_min] = silhouette_score(data, kmedoids.labels_, metric = mixed_distance, random_state = seed)\n","    ch_score[k - k_min] = calinski_harabasz_score(data, kmedoids.labels_)\n","    db_score[k - k_min] = davies_bouldin_score(data, kmedoids.labels_)"]},{"attachments":{},"cell_type":"markdown","id":"428a96e9","metadata":{},"source":["Plotting silhouette, Calinski-Harabasz and Davies-Bouldin metrics as a function of <code>k</code>"]},{"cell_type":"code","execution_count":null,"id":"5b380d12","metadata":{},"outputs":[],"source":["fig, axes = plt.subplots(1, 3, figsize=(18,5), dpi=80)\n","ax = axes[0]\n","ax.plot(k_vector, silhouette, 'o--', color = '#377eb8')\n","ax.set_xlabel('$k$')\n","ax.set_title('Silhouette score')\n","\n","ax = axes[1]\n","ax.plot(k_vector, ch_score, 'o--', color = '#ff7f00')\n","ax.set_xlabel('$k$')\n","ax.set_title('Calinski-Harabasz score')\n","\n","ax = axes[2]\n","ax.plot(k_vector, db_score, 'o--', color = '#4daf4a')\n","ax.set_xlabel('$k$')\n","ax.set_title('Davies-Bouldin score')\n","\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","id":"93bc7d89","metadata":{},"source":["<code>k = 5</code> is selected as the best <code>k</code> for k-medoids according to the above scores.\n","\n","Results are plotted in t-SNE representation with <code>perplexity = 45</code> and <code>learning_rate = 200</code>."]},{"cell_type":"code","execution_count":null,"id":"1b278aae","metadata":{},"outputs":[],"source":["from sklearn.manifold import TSNE\n","\n","# selecting k_bar = 4 as best k\n","k_bar = 4\n","\n","# defining the values for learning rate and perplexity that are going to be used for t-SNE\n","perplexity = 45\n","learning_rate = 200\n","dimensions = 2\n","\n","# running t-SNE to have a 2D data representation\n","tsne = TSNE(n_components=dimensions,\n","            perplexity=perplexity,\n","            learning_rate=learning_rate,\n","            metric=mixed_distance,\n","            random_state=seed)\n","\n","X_tsne = tsne.fit_transform(data)\n","\n","# running kmedoids for k = k_bar\n","kmedoids = KMedoids(n_clusters = k_bar, metric = mixed_distance, random_state = seed).fit(data)\n","\n","# defining the 'colors' list that is going to be used to assign a different color to each data point in the plot\n","color_list = ['#377eb8', '#ff7f00', '#4daf4a', \"#f781bf\", \"#a65628\", \"#984ea3\"]\n","colors = []\n","for i in kmedoids.labels_:\n","   colors.append(color_list[i])\n","\n","# plotting the clusters in t-SNE representation\n","fig = plt.figure(figsize=(8,8))\n","plt.scatter(X_tsne[:, 0], X_tsne[:, 1], color=colors, edgecolors='black')\n","plt.set_title(f'Lr = {learning_rate}, P = {perplexity}, $k$ = {k_bar}')\n"]},{"cell_type":"code","execution_count":null,"id":"d403f4bc","metadata":{},"outputs":[],"source":["tsne = TSNE(n_components=3,\n","            perplexity=perplexity,\n","            learning_rate=learning_rate,\n","            metric=mixed_distance,\n","            random_state=seed).fit(data)\n","\n","tsne = tsne.embedding_\n","fig = plt.figure(figsize=(8,8))\n","ax = plt.axes(projection='3d')\n","_ = ax.scatter(tsne[:,0], tsne[:,1], tsne[:,2], 'o', color=colors, edgecolors='black')"]},{"attachments":{},"cell_type":"markdown","id":"9f36ea13","metadata":{},"source":["## Other methods"]},{"attachments":{},"cell_type":"markdown","id":"8c156b19","metadata":{},"source":["### DBSCAN"]},{"attachments":{},"cell_type":"markdown","id":"f116a427","metadata":{},"source":["Density-based spatial clustering of applications with noise (DBSCAN) is a density-based clustering non-parametric algorithm: given a set of points in some space, it groups together points that are closely packed together (points with many nearby neighbors), marking as outliers points that lie alone in low-density regions (whose nearest neighbors are too far away).\n","\n","A fundamental hyperparameter in DBSCAN clustering is <code>eps</code>, which is the maximum distance between two samples for one to be considered as in the neighborhood of the other. To have an idea of the order of magnitude of this value, the average distance between all data points using <code>mixed_metric</code> is computed."]},{"cell_type":"code","execution_count":null,"id":"0be29622","metadata":{},"outputs":[],"source":["# average distance is computed as the sum of distances between all samples divided by the number of all the distances\n","# sum of all distances is initialized to zero (float is used to not lose decimals during the computation of avg_distance)\n","distances_sum = 0.0\n","\n","# the total number of distances of between samples (n = data.shape[0] in this case) is n(n+1)/2\n","distances_number = (data.shape[0]) * (data.shape[0] + 1.0) / 2.0\n","\n","# updating the sum of distances for each couple of points\n","for i in range(data.shape[0]):\n","    for j in range(i+1, data.shape[0]):\n","        distances_sum = distances_sum + mixed_distance(data[i], data[j])[0][0]  # [0][0] is used to pick the distance value, since the output is a 2D array\n","\n","avg_distance = distances_sum / distances_number\n","print(avg_distance)"]},{"attachments":{},"cell_type":"markdown","id":"c85a9e85","metadata":{},"source":["Since this algorithm is not performing too well on the data, filtering was applied in order to remove inconsistent results. <code>[Continue here the discussion on the filtering]</code>"]},{"cell_type":"code","execution_count":null,"id":"196a79e6","metadata":{},"outputs":[],"source":["from math import floor\n","from sklearn.cluster import DBSCAN\n","from sklearn.metrics import silhouette_score\n","from sklearn.metrics import calinski_harabasz_score\n","from sklearn.metrics import davies_bouldin_score\n","\n","# defining the possible eps values (ranging from 0.5 to 1.1 with step size 0.2, the default value is 0.5)\n","initial_eps = 0.5\n","final_eps = 1.3\n","step_eps = 0.2\n","eps_vector = np.arange(initial_eps, final_eps + step_eps, step_eps)\n","\n","# defining the possible min_samples values (ranging from 3 to 9 with step size 2, the default value is 5)\n","initial_min_samples = 5\n","final_min_samples = 13\n","step_min_samples = 2\n","min_samples_vector = np.arange(initial_min_samples, final_min_samples + step_min_samples, step_min_samples)\n","\n","# defining the empty lists that will contain DBSCAN filtered parameters and results (inconsistent results are not stored)\n","eps_list = []\n","min_samples_list = []\n","unique_labels_list = []\n","outliers_number_list = []\n","silhouette_list = []\n","db_list = []\n","ch_list = []\n","\n","# defining the array that will store the output labels of the algorithm\n","dbscan_labels = np.empty((eps_vector.shape[0] * min_samples_vector.shape[0], data.shape[0]))\n","success_count = 0\n","\n","# defining the maximum allowed outliers percentage as 50%\n","outliers_threshold = 50\n","\n","# running DBSCAN and store the results for each value of eps and min_samples\n","for eps in eps_vector:\n","    for min_samples in min_samples_vector:\n","\n","        # store the labels in temp_labels using DBASCAN fit_predict method\n","        temp_labels = DBSCAN(eps=eps, min_samples=min_samples, metric=mixed_distance, p=1).fit_predict(data)\n","\n","        # defining the outliers filters on data (boolean arrays with the same lenght of the labels array temp_labels)\n","        is_outlier = temp_labels == -1\n","        is_not_outlier = temp_labels != -1\n","\n","        # determining the number of clusters produced by DBSCAN\n","        # np.unique(v) generates a vector from v containing only once the values contained in v\n","        # in this case v is the labels vector filtered from the outliers (using the filter is_not_outlier)\n","        unique_labels = np.unique(temp_labels[is_not_outlier]).shape[0]\n","\n","        # calculating the rounded down percentage of outliers with respect to the total number of data points\n","        # float casting is used to not lose decimals during the division\n","        outliers_percentage = floor(float(temp_labels[is_outlier].shape[0]) / float(data.shape[0]) * 100.0)\n","\n","        # the results are not considered meaningful if: \n","        # - the number of clusters generated by DBSCAN is less than 3\n","        # - the number of clusters generated by DBSCAN is more than 9\n","        # - the outliers percentage exceeds the threshold of 50%\n","        # if these conditions are not met, the loop skips to next iteration without appending the results in the lists (using continue)\n","        if (unique_labels < 3) or (unique_labels > 9) or (outliers_percentage > outliers_threshold):\n","            continue\n","        \n","        # appending the parameters and the scores to the lists\n","        eps_list.append(eps)\n","        min_samples_list.append(min_samples)\n","        outliers_number_list.append(outliers_percentage)\n","        unique_labels_list.append(unique_labels)\n","\n","        silhouette_list.append(silhouette_score(data[is_not_outlier], \n","                                                temp_labels[is_not_outlier], \n","                                                metric = mixed_distance, \n","                                                random_state = seed))\n","        \n","        db_list.append(davies_bouldin_score(data[is_not_outlier], \n","                                            temp_labels[is_not_outlier]))\n","        \n","        ch_list.append(calinski_harabasz_score(data[is_not_outlier], \n","                                               temp_labels[is_not_outlier]))\n","        \n","        # storing the labels in the respective array\n","        dbscan_labels[success_count, :] = np.copy(temp_labels)\n","        success_count = success_count + 1"]},{"attachments":{},"cell_type":"markdown","id":"b4540ac1","metadata":{},"source":["Printing the values of the parameters related to the filtered outputs."]},{"cell_type":"code","execution_count":null,"id":"ae295163","metadata":{},"outputs":[],"source":["print(f'eps values: {eps_list}')\n","print(f'min_samples values: {min_samples_list}')\n","print(f'outliers_number values: {outliers_number_list}')\n","print(f'unique_labels values: {unique_labels_list}')"]},{"attachments":{},"cell_type":"markdown","id":"0d087c46","metadata":{},"source":["Plotting silhouette, Calinski-Harabasz and Davies-Bouldin scores as a function of <code>eps</code> and <code>min_samples</code> hyperparameters, showing also the outlier percentages."]},{"cell_type":"code","execution_count":null,"id":"8a71b36e","metadata":{},"outputs":[],"source":["# defining the object containing the plots\n","fig, axes = plt.subplots(1, 3, figsize=(18,5), dpi=80)\n","\n","# creating a list containing the ordered unique values of eps, used to plot the scores\n","eps_unique = sorted(list(set(eps_list)), key=float)\n","\n","# defining a color list to distinguish between different eps values in the plots\n","color_list = ['#377eb8', '#ff7f00', '#4daf4a', \"#f781bf\", \"#a65628\", \"#984ea3\"]\n","\n","# converting lists to numpy arrays in order to exploit filtering\n","eps_list = np.array(eps_list)\n","min_samples_list = np.array(min_samples_list)\n","outliers_number_list = np.array(outliers_number_list)\n","silhouette_list = np.array(silhouette_list)\n","ch_list = np.array(ch_list)\n","db_list = np.array(db_list)\n","\n","# initializing the counter for unique values of eps (used to color the plots)\n","eps_count = 0\n","\n","# for each unique value of eps, plot the three scores onto three different plots (each plot is a 'o--' scatterplot)\n","for eps in eps_unique:\n","    filt = (eps_list == eps)\n","    axes[0].plot(min_samples_list[filt], silhouette_list[filt], 'o--', color = color_list[eps_count], label = f'eps = {eps}')\n","    axes[1].plot(min_samples_list[filt], ch_list[filt], 'o--', color = color_list[eps_count], label = f'eps = {eps}')\n","    axes[2].plot(min_samples_list[filt], db_list[filt], 'o--', color = color_list[eps_count], label = f'eps = {eps}')\n","    eps_count = eps_count + 1\n","\n","# for each point, annotate the percentage of outliers on the plots\n","for i in range(len(eps_list)):\n","    axes[0].annotate(outliers_number_list[i], (min_samples_list[i], silhouette_list[i]))\n","    axes[1].annotate(outliers_number_list[i], (min_samples_list[i], ch_list[i]))\n","    axes[2].annotate(outliers_number_list[i], (min_samples_list[i], db_list[i]))\n","\n","# for each plot, insert the legend\n","for i in range(3):\n","    axes[i].legend()\n","    axes[i].set_xlabel('min_samples')\n","\n","# setting plot titles\n","_ = axes[0].set_title('Silhouette score')\n","_ = axes[1].set_title('Calinski-Harabasz score')\n","_ = axes[2].set_title('Davies-Bouldin score')\n"]},{"attachments":{},"cell_type":"markdown","id":"8c671e28","metadata":{},"source":["Plotting DBSCAN clustering results in 2D t-SNE representation (using <code>perplexity = 45</code> and <code>learning_rate = 200</code>)."]},{"cell_type":"code","execution_count":null,"id":"aecf2091","metadata":{},"outputs":[],"source":["from sklearn.manifold import TSNE\n","\n","# defining the color list (black is the last one and represents outiers)\n","color_list = [\"#377eb8\", \"#ff7f00\", \"#4daf4a\", \"#f781bf\", \"#a65628\", \"#984ea3\", \"#999999\", \"#e41a1c\", \"#dede00\", 'black']\n","\n","# during DBSCAN, success_count was counting the number of significative outputs for different eps and min_samples values\n","plots_number = success_count\n","\n","# defining the number of rows, number of columns and the object containing the plots\n","rows = int((plots_number + 2) / 3)\n","columns = min(3, plots_number)\n","fig, axes = plt.subplots(rows, columns, figsize=(6*columns, 5*rows), dpi=80)\n","\n","# applying t-sne to data and storing the transformed data in X_tsne\n","tsne = TSNE(n_components=2,\n","            perplexity=45,\n","            learning_rate=200,\n","            metric=mixed_distance,\n","            random_state=seed)\n","X_tsne = tsne.fit_transform(data)\n","\n","for i in range(plots_number):\n","\n","   # selecting the right subplot\n","   ax = axes[int(i/3), i%3]\n","\n","   # selecting the right labels\n","   labels = dbscan_labels[i, :]\n","\n","   # constructing the list containing the color associated to each labeled point\n","   colors = []\n","   for l in labels:\n","      colors.append(color_list[int(l)])\n","\n","   # plotting the points with the respective colors and setting the title of each plot\n","   ax.scatter(X_tsne[:, 0], X_tsne[:, 1], color=colors)\n","   _ = ax.set_title(f'eps = {eps_list[i]}, min_samples = {min_samples_list[i]}, outliers = {outliers_number_list[i]}%')"]},{"attachments":{},"cell_type":"markdown","id":"1cb1c255","metadata":{},"source":["### OPTICS"]},{"attachments":{},"cell_type":"markdown","id":"19598fa4","metadata":{},"source":["Ordering points to identify the clustering structure (OPTICS) is an algorithm for finding density-based clusters in spatial data. Its basic idea is similar to DBSCAN, but it addresses one of DBSCAN's major weaknesses: the problem of detecting meaningful clusters in data of varying density. To do so, the points of the database are (linearly) ordered such that spatially closest points become neighbors in the ordering. Additionally, a special distance is stored for each point that represents the density that must be accepted for a cluster so that both points belong to the same cluster.\n","\n","The procedure is the same used for DBSCAN clustering."]},{"cell_type":"code","execution_count":null,"id":"6d0bc668","metadata":{},"outputs":[],"source":["from math import floor\n","from sklearn.cluster import OPTICS\n","\n","initial_eps = 0.4\n","final_eps = 0.6\n","step_eps = 0.1\n","\n","initial_min_samples = 4\n","final_min_samples = 7\n","step_min_samples = 1\n","\n","eps_vector = np.arange(initial_eps, final_eps + step_eps, step_eps)\n","min_samples_vector = np.arange(initial_min_samples, final_min_samples + step_min_samples, step_min_samples)\n","\n","silhouette_list = []\n","db_list = []\n","ch_list = []\n","outliers_number_list = []\n","eps_list = []\n","min_samples_list = []\n","outliers_threshold = 95\n","optics_labels = np.empty((eps_vector.shape[0] * min_samples_vector.shape[0], data.shape[0]))\n","count = 0\n","\n","for eps in eps_vector:\n","    for min_samples in min_samples_vector:\n","        temp_labels = OPTICS(eps=eps, min_samples=min_samples, metric=mixed_distance).fit_predict(data)\n","\n","        is_outlier = temp_labels == -1\n","        is_not_outlier = temp_labels != -1\n","        unique_labels = np.unique(temp_labels[is_not_outlier]).shape[0]\n","        # not good labels = only one class or more than ten classes\n","        is_not_good_labels = (unique_labels < 2) or (unique_labels > 10)\n","        outliers_percentage = floor(float(temp_labels[is_outlier].shape[0]) / float(data.shape[0]) * 100.0)\n","\n","        if is_not_good_labels or (outliers_percentage > outliers_threshold):\n","            continue\n","\n","        eps_list.append(eps)\n","        min_samples_list.append(min_samples)\n","        outliers_number_list.append(outliers_percentage)\n","        optics_labels[count, :] = np.copy(temp_labels)\n","        count = count + 1\n","\n","        silhouette_list.append(silhouette_score(data[is_not_outlier], \n","                                                temp_labels[is_not_outlier], \n","                                                metric = mixed_distance, \n","                                                random_state = seed))\n","        \n","        db_list.append(davies_bouldin_score(data[is_not_outlier], \n","                                            temp_labels[is_not_outlier]))\n","        \n","        ch_list.append(calinski_harabasz_score(data[is_not_outlier], \n","                                               temp_labels[is_not_outlier]))"]},{"cell_type":"code","execution_count":null,"id":"3de63311","metadata":{},"outputs":[],"source":["print(eps_list)\n","print(min_samples_list)"]},{"cell_type":"code","execution_count":null,"id":"a760486d","metadata":{},"outputs":[],"source":["\"\"\" fig, axes = plt.subplots(1, 3, figsize=(18,5), dpi=80)\n","ax = axes[0]\n","ax.plot(min_samples_list[:4], silhouette_list[:4], 'o--', color = '#377eb8', label = '$eps=0.4$')\n","ax.plot(min_samples_list[:4], silhouette_list[4:8], 'o--', color = '#ff7f00', label = '$eps=0.5$')\n","ax.set_xlabel('min_samples')\n","ax.set_ylabel('Silhouette score')\n","for i in range(8):\n","    ax.annotate(outliers_number_list[i], (min_samples_list[i], silhouette_list[i]))\n","ax.legend()\n","\n","ax = axes[1]\n","ax.plot(min_samples_list[:4], ch_list[:4], 'o--', color = '#377eb8', label = '$eps=0.4$')\n","ax.plot(min_samples_list[:4], ch_list[4:8], 'o--', color = '#ff7f00', label = '$eps=0.5$')\n","ax.set_xlabel('min_samples')\n","ax.set_ylabel('Calinski-Harabasz score')\n","for i in range(8):\n","    ax.annotate(outliers_number_list[i], (min_samples_list[i], ch_list[i]))\n","ax.legend()\n","\n","ax = axes[2]\n","ax.plot(min_samples_list[:4], db_list[:4], 'o--', color = '#377eb8', label = '$eps=0.4$')\n","ax.plot(min_samples_list[:4], db_list[4:8], 'o--', color = '#ff7f00', label = '$eps=0.5$')\n","ax.set_xlabel('min_samples')\n","ax.set_ylabel('Davies-Bouldin score')\n","for i in range(8):\n","    ax.annotate(outliers_number_list[i], (min_samples_list[i], db_list[i]))\n","ax.legend()\n","\n","plt.show() \"\"\""]},{"cell_type":"code","execution_count":null,"id":"1b3f70d6","metadata":{},"outputs":[],"source":["\"\"\" labels = optics_labels[, :]\n","is_not_outlier = labels != -1\n","unique_labels = np.unique(labels[is_not_outlier]).shape[0]\n","print(unique_labels) \"\"\""]},{"cell_type":"code","execution_count":null,"id":"bd32ddef","metadata":{},"outputs":[],"source":["\"\"\" from sklearn.manifold import TSNE\n","\n","color_list = ['#377eb8', '#ff7f00', '#4daf4a', '#f781bf', '#a65628', 'black']\n","colors = []\n","for i in labels:\n","   colors.append(color_list[i])\n","\n","tsne = TSNE(n_components=2,\n","            perplexity=35,\n","            learning_rate=480,\n","            metric=mixed_distance,\n","            random_state=seed)\n","X_tsne = tsne.fit_transform(data)\n","plt.scatter(X_tsne[:, 0], X_tsne[:, 1], color=colors)\n","plt.title(f'Lr = {learning_rate} - P = {perplexity}') \"\"\""]},{"attachments":{},"cell_type":"markdown","id":"e958c058","metadata":{},"source":["### Gaussian Mixture Model"]},{"attachments":{},"cell_type":"markdown","id":"34f5fd75","metadata":{},"source":["Gaussian Mixture Model (GMM) is a clustering algorithm that assumes data is generated by a combination of gaussian distributions. The goal of the algorithm is to estimate the parameters of these distributions to determine to which cluster each point belongs.\n","\n","Clustering with GMM starts with the initialization of the parameters. These include the means and the covariance matrices of the gaussian distributions and the weights that represent the fraction of data points belonging to each distribution. Then the algorithm loops two phases: the assignment of data points to the clusters and the estimation of the parameters. In the first one, each point is assigned to the cluster whose gaussian distribution has the highest probability of generating the point. In the second phase the parameters of the distributions are estimated using the points belonging to each cluster. This procedure is repeated until convergence."]},{"cell_type":"code","execution_count":null,"id":"2d47b687","metadata":{},"outputs":[],"source":["from sklearn.metrics import silhouette_score\n","from sklearn.metrics import calinski_harabasz_score\n","from sklearn.metrics import davies_bouldin_score\n","from sklearn.mixture import GaussianMixture\n","\n","k_min = 2\n","k_max = 6\n","k_vector = np.arange(k_min, k_max + 1, 1)\n","silhouette = np.empty(k_vector.shape)\n","ch_score = np.empty(k_vector.shape)\n","db_score = np.empty(k_vector.shape)\n","for k in k_vector:\n","    # training Gaussian Mixture mdodel\n","    gmm = GaussianMixture(n_components = k, random_state = seed, init_params = 'kmeans').fit(data)\n","    # defining the labels\n","    labels = gmm.predict(data)\n","    silhouette[k - k_min] = silhouette_score(data, labels, metric = mixed_distance, random_state = seed)\n","    ch_score[k - k_min] = calinski_harabasz_score(data, labels)\n","    db_score[k - k_min] = davies_bouldin_score(data, labels)"]},{"attachments":{},"cell_type":"markdown","id":"e272f72f","metadata":{},"source":["Plotting silhouette, Calinski-Harabasz and Davies-Bouldin metrics as a function of <code>k</code>"]},{"cell_type":"code","execution_count":null,"id":"7a0813ed","metadata":{},"outputs":[],"source":["fig, axes = plt.subplots(1, 3, figsize=(18,5), dpi=80)\n","ax = axes[0]\n","ax.plot(k_vector, silhouette, 'o--', color = '#377eb8')\n","ax.set_xlabel('$k$')\n","ax.set_ylabel('Silhouette score')\n","\n","ax = axes[1]\n","ax.plot(k_vector, ch_score, 'o--', color = '#ff7f00')\n","ax.set_xlabel('$k$')\n","ax.set_ylabel('Calinski-Harabasz score')\n","\n","ax = axes[2]\n","ax.plot(k_vector, db_score, 'o--', color = '#4daf4a')\n","ax.set_xlabel('$k$')\n","ax.set_ylabel('Davies-Bouldin score')\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"98d8df96","metadata":{},"outputs":[],"source":["from sklearn.manifold import TSNE\n","from sklearn.mixture import GaussianMixture\n","\n","k_min = 2\n","k_max = 6\n","k_vector = np.arange(k_min, k_max + 1, 1)\n","\n","fig, axes = plt.subplots(2, 3, figsize=(18,8), dpi=80)\n","row = 0\n","column = 0\n","\n","\n","for k in k_vector:\n","  gmm = GaussianMixture(n_components = k, random_state = seed, init_params = 'random').fit(data)\n","  labels = gmm.predict(data)\n","  color_list = ['#377eb8', '#ff7f00', '#4daf4a', '#f781bf', '#a65628', 'black']\n","  colors = []\n","  for i in labels:\n","     colors.append(color_list[i])\n","\n","  tsne = TSNE(n_components=2,\n","              perplexity=35,\n","              learning_rate=480,\n","              metric=mixed_distance,\n","              random_state=seed)\n","  \n","  X_tsne = tsne.fit_transform(data)\n","  ax = axes[row][column]\n","  ax.scatter(X_tsne[:, 0], X_tsne[:, 1], color=colors)\n","  ax.set_title(f'k = {k}')\n","  column = column + 1\n","  if column % 3 == 0:\n","    column = 0\n","    row = row + 1 \n","\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","id":"0185b97b","metadata":{},"source":["# Clustering results"]},{"attachments":{},"cell_type":"markdown","id":"af685a29","metadata":{},"source":["The clustering method which performed best among the ones tried out was k-medoids clustering. The following analysis is based on this method's results.\n","Before interpreting the results, the algorithm is run on the whole data."]},{"cell_type":"code","execution_count":null,"id":"926b3a14","metadata":{},"outputs":[],"source":["data = df.sample(frac = 1, random_state = seed).to_numpy()\n","\n","kmedoids = KMedoids(n_clusters = k_bar, metric = mixed_distance, random_state = seed).fit(data)\n","\n","labels = kmedoids.labels_\n","unique_labels = set(labels)\n","filtered_df = pd.DataFrame(data)\n","filtered_df.columns = df.columns\n","filtered_df['Labels'] = labels"]},{"attachments":{},"cell_type":"markdown","id":"4373b5ef","metadata":{},"source":["Numerical features are plotted in order to study clusters distributions."]},{"cell_type":"code","execution_count":null,"id":"a87072c4","metadata":{},"outputs":[],"source":["colour = sns.color_palette(\"hls\", k_bar)\n","\n","sns.pairplot(filtered_df, \n","             hue= 'Labels', \n","             corner = True, \n","             palette = colour, \n","             vars = ['Age', 'FamilySize', 'Income', 'Wealth', 'Debt', 'FinEdu', 'ESG', 'Digital', 'BankFriend', 'LifeStyle', 'Luxury', 'Saving'])\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","id":"606c8be3","metadata":{},"source":["The histogram showing the number of points per cluster is plotted."]},{"cell_type":"code","execution_count":null,"id":"fecfe91b","metadata":{},"outputs":[],"source":["centers = kmedoids.cluster_centers_\n","\n","counts = np.zeros(k_bar)\n","\n","for i in range(k_bar):\n","  counts[i] = len(kmedoids.labels_[kmedoids.labels_ == i])\n","\n","x_labels = ['Cluster {}'.format(i+1) for i in range(len(counts))]\n","y_label = 'Absolute frequncies'\n","title = 'Cluster density'\n","\n","sns.barplot(x = x_labels, y=counts)\n","\n","plt.ylabel(y_label)\n","plt.title(title)\n","\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","id":"c3f4e7ee","metadata":{},"source":["<code>Feature importance analysis with xgboost</code>"]},{"cell_type":"code","execution_count":null,"id":"8e89d277","metadata":{},"outputs":[],"source":["y_cluster = np.zeros((len(kmedoids.labels_), k_bar))\n","features = df.columns\n","\n","row = 0\n","column = 0\n","max_rows = 3\n","max_columns = 2\n","fig, axes = plt.subplots(max_rows, max_columns, figsize=(16,24), dpi=80)\n","\n","summary = pd.DataFrame()\n","summary.index = features\n","\n","for i in range(k_bar):\n","    y_cluster[kmedoids.labels_ == i, i] = 1\n","    temp_data = pd.DataFrame(data[y_cluster[:, i] == 1,:], columns=features)\n","    temp_model = xgboost.XGBClassifier()\n","    temp_model.fit(data, y_cluster[:, i])\n","    temp_model.get_booster().feature_names = list(features)\n","    summary[f'Cluster{i+1}'] = np.mean(temp_data, axis=0)\n","\n","    ax = axes[row][column]\n","\n","    xgboost.plot_importance(temp_model.get_booster(), title=f'Cluster {i+1}', ylabel='', ax = ax)\n","  \n","    column = column + 1\n","    if column % 2 == 0:\n","        column = 0\n","        row = row + 1 "]},{"cell_type":"code","execution_count":null,"id":"b1b0374f","metadata":{},"outputs":[],"source":["display(summary)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"}},"nbformat":4,"nbformat_minor":5}
